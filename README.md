# üß† Visual Question Answering (VQA) avec BLIP sur VizWiz

Ce projet montre comment fine-tuner le mod√®le pr√©-entra√Æn√© **BLIP-VQA-base** sur le jeu de donn√©es **VizWiz-VQA**, afin de permettre √† un syst√®me de r√©pondre √† des questions en langage naturel pos√©es sur des images prises dans des conditions r√©elles (par des personnes malvoyantes).

---

## üñºÔ∏è Sch√©ma d‚Äôarchitecture

![BLIP-VQA pipeline](./A_diagram_titled_"BLIP_Fine-Tuning_for_VQA_on_VizW.png)

---

## üß∞ Stack utilis√©e

| Composant        | R√¥le |
|------------------|------|
| `BLIP-VQA-base`  | Mod√®le multimodal vision + texte (ViT + BERT) |
| `VizWiz`         | Dataset VQA d‚Äôimages r√©elles avec annotations humaines |
| `PyTorch`        | Framework de deep learning |
| `Transformers`   | Chargement du mod√®le BLIP via ü§ó Hugging Face |
| `Streamlit`      | Interface web pour tester en ligne |
| `Google Colab`   | Environnement d‚Äôentra√Ænement (12 Go de VRAM) |

---

## üóÇÔ∏è Pipeline de traitement

1. **Extraction** : chargement manuel des images et des JSON contenant les questions / r√©ponses.
2. **Pr√©traitement √† la vol√©e** : `BlipProcessor` transforme les images (224√ó224 RGB) et tokenize les questions.
3. **Fine-tuning adaptatif** :
   - le **Vision Encoder (ViT)** est gel√© au d√©but,
   - seuls l‚Äôencodeur texte, la fusion et la t√™te g√©n√©rative sont entra√Æn√©s,
   - si la perte stagne, le ViT est progressivement d√©gel√©.
4. **√âvaluation** : pr√©diction par g√©n√©ration token-par-token + calcul d‚Äôaccuracy sur les r√©ponses humaines.
5. **D√©ploiement** : interface interactive avec Streamlit.

---

## ‚ñ∂Ô∏è D√©mo Streamlit (locale)

```bash
streamlit run app.py
